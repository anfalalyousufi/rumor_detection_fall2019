{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project -fall2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Installed:\n",
    "##### tweepy, json, preprocessor, os, time, codecs\n",
    "\n",
    "\n",
    "#### pip install tweet-preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Posting ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24753438\n"
     ]
    }
   ],
   "source": [
    "import tweepy\n",
    "import json\n",
    "\n",
    "#add users credentials\n",
    "consumer_key = \"RmYMHj3RwVL2qj2sB9V0jbGB2\"\n",
    "consumer_secret = \"rwkLatJck2pvHEeljerwWfwYi6Wf0Tquy90UyERunzFeXCAlW7\"\n",
    "access_key = \"590766984-MJRgfipJLzhQt2R6VcktfsANXtiH6Rs87MUpl6ly\"\n",
    "access_secret = \"P6hl0Lwfclbcz4JjAWb9ILrTkyrPGB6cpd0UVGKmgGcZU\"\n",
    " \n",
    "def get_tweets(id1):\n",
    "        # Authorization to consumer key and consumer secret\n",
    "        auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "        # Access to user's access key and access secret\n",
    "        auth.set_access_token(access_key, access_secret)\n",
    "        # Calling api\n",
    "        api=tweepy.API(auth,retry_count=3,retry_delay=15,retry_errors=set([401, 404, 500, 503]), wait_on_rate_limit_notify=False ,wait_on_rate_limit=True,)\n",
    "        stat=api.get_status(id1)\n",
    "        return stat.user.id\n",
    "# Driver code\n",
    "if __name__ == '__main__':\n",
    " \n",
    "    # Here goes the twitter handle for the user\n",
    "    # whose tweets are to be extracted.\n",
    "    #get_tweets(\"SalmanKhan_\")\n",
    "    print(get_tweets(\"260244087901413376\"));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#39Indians.txt\t\t5800\n",
      "#AadhaarMythBuster.txt\t\t3100\n",
      "#arushi.txt\t\t2100\n",
      "#BJPCambridgeAnalyticaNexus.txt\t\t2563\n",
      "#DeleteNamoApp.txt\t\t6339\n",
      "#DemocracyinDanger.txt\t\t14300\n",
      "#DestroyTheAadhaar.txt\t\t4375\n",
      "#gst.txt\t\t5468\n",
      "#JusticeLoya.txt\t\t4086\n",
      "#Noteban.txt\t\t2616\n",
      "#PNBfraud.txt\t\t2200\n",
      "#Pradyuman.txt\t\t1311\n",
      "#RafelScam.txt\t\t246\n",
      "#SaveTheInternet.txt\t\t304\n",
      "#SSCScam.txt\t\t243\n",
      "10coinInvalid.txt\t\t28\n",
      "2000 note close .txt\t\t393\n",
      "AMUservingfoodhindu.txt\t\t33\n",
      "azadi lakh army Kashmir.txt\t\t110\n",
      "ben kingsley indian.txt\t\t1184\n",
      "Bitcoin ban india.txt\t\t3076\n",
      "BJP’sNupurSharmaAskstoSaveBengalUsing2002RiotsImage.txt\t\t12\n",
      "call 777888999.txt\t\t946\n",
      "caste public discourse .txt\t\t82\n",
      "chennai airport flood .txt\t\t512\n",
      "conversion rate card.txt\t\t183\n",
      "CPM cyber warriors.txt\t\t76\n",
      "dawood property SEIZED.txt\t\t828\n",
      "dhoni liters milk  .txt\t\t19\n",
      "Dhoni Match fixing .txt\t\t2933\n",
      "dhyanchand german.txt\t\t59\n",
      "diamondFirstMinedIndia.txt\t\t320\n",
      "diwali photo space.txt\t\t323\n",
      "dyingWomanMolested.txt\t\t305\n",
      "ElectricityJamaMasjid.txt\t\t636\n",
      "EVMHack .txt\t\t290\n",
      "facebookBff.txt\t\t3078\n",
      "FatwaMenEatWives.txt\t\t1136\n",
      "firstRocketIndiaTransportedCycle.txt\t\t111\n",
      "Freddie Mercury indian.txt\t\t836\n",
      "ganesh drinking milk.txt\t\t237\n",
      "goonchKaliRiver.txt\t\t48\n",
      "gps 2000note .txt\t\t21\n",
      "havells indian brand.txt\t\t16\n",
      "hindi official language.txt\t\t6797\n",
      "hindiOnlyOfficialLanguage.txt\t\t1090\n",
      "india no national game.txt\t\t680\n",
      "india spa elephants.txt\t\t503\n",
      "india women save fuel.txt\t\t27\n",
      "indiaSpaElephants.txt\t\t506\n",
      "inidan govt block porn site .txt\t\t4\n",
      "irfan khan cancer .txt\t\t562\n",
      "itc tobacco company.txt\t\t333\n",
      "JasleenKaurJosanFirstIndianAstronautToJointhe2030MarsMission.txt\t\t1\n",
      "Jayalalitha secret daughter.txt\t\t14\n",
      "JeanDrèzeIndian.txt\t\t241\n",
      "kfc vegetarian for india.txt\t\t85\n",
      "kirpana destroyed.txt\t\t51\n",
      "kovind million followers.txt\t\t182\n",
      "kumbhMelaFromSpace.txt\t\t817\n",
      "lalu daughter harvard lecture.txt\t\t105\n",
      "largest vegetarians india.txt\t\t413\n",
      "lulia salman marriage .txt\t\t634\n",
      "mahatma gandhi dancing images.txt\t\t394\n",
      "mia khalifa bollywood.txt\t\t613\n",
      "milkha looked back .txt\t\t24\n",
      "muslimGirlWonGitaContest.txt\t\t136\n",
      "Muslims arrested Celebrating Win .txt\t\t39\n",
      "Muslims Celebrating pak Win.txt\t\t117\n",
      "NetNeutrality india.txt\t\t5442\n",
      "Nostradamus predicted modi.txt\t\t1022\n",
      "Paresh Mesta murder.txt\t\t457\n",
      "plastic cabbage india.txt\t\t42\n",
      "ponting bat spring .txt\t\t786\n",
      "PrayForChristianPrannoyRoy.txt\t\t6\n",
      "RaveenaTandonToNawazSharif.txt\t\t37\n",
      "Robert Vadra chinese envoy.txt\t\t86\n",
      "salman katrina  married .txt\t\t833\n",
      "salt shortage india.txt\t\t293\n",
      "samsung 9 under display fingerprint.txt\t\t639\n",
      "Senate Elections .txt\t\t345\n",
      "shampooing Indian Concept.txt\t\t93\n",
      "shashi tharoor killed wife.txt\t\t2240\n",
      "SnakesLadderOriginatedIndia.txt\t\t298\n",
      "snapchat snapdeal same.txt\t\t193\n",
      "sridevi death twist .txt\t\t225\n",
      "surya missile .txt\t\t107\n",
      "templeOfRats.txt\t\t3546\n",
      "TripleTalaq .txt\t\t1900\n",
      "under Sasikala Home.txt\t\t25\n",
      "UNESCO Jana Gana Mana best national anthem.txt\t\t852\n",
      "UNESCO Modi best Prime Minister.txt\t\t216\n",
      "VideoClaimingBengaluruAirportWasFlooded.txt\t\t4\n",
      "village name pakistan bihar.txt\t\t23\n",
      "waterMoonDiscoveredIndia.txt\t\t51\n",
      "WhatsAppprofilepictureISIS.txt\t\t7\n",
      "worm dairy milk.txt\t\t411\n",
      "yogi urine.txt\t\t575\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import preprocessor as p\n",
    "#import preprocess as p\n",
    "import sys\n",
    "\n",
    "def process(lines,filenames):\n",
    "    with open(os.getcwd()+\"\\IndianData/\"+filename, \"w+\", encoding='utf-8') as temp:\n",
    "        s1=set()\n",
    "        for i in lines[:]:\n",
    "            if(len(i)>30):\n",
    "                s=i.split('\\t')\n",
    "                s1.add(str(s[1]))\n",
    "        #print(s1)\n",
    "        for i in lines[:]:\n",
    "            if(len(i)>30):\n",
    "                s=i.split('\\t')\n",
    "                temp.write(s[0]+'\\t'+s[1]+\"\\t\"+s[2]+'\\t'+p.clean(s[3])+'\\n')\n",
    "                \n",
    "for filename in os.listdir(os.getcwd()+'/indian Rumor'):\n",
    "    #print(filename)\n",
    "    with open(\"./indian Rumor/\"+filename,'r', encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "        print(filename+'\\t\\t'+str(int(len(lines)/2)))\n",
    "        process(lines,filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Friends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[830360126347309056, 13058232, 48410093, 131249511, 135421739, 87170183, 57928790, 19929890, 18681139, 53555106, 97865628, 56631494, 70179948, 24705126, 101695592, 53790896, 86254626, 27602673, 17753033, 113419517, 114501238, 75854417, 104740065, 87118217]\n"
     ]
    }
   ],
   "source": [
    "import tweepy\n",
    "import time\n",
    "\n",
    "def friends(UserId):\n",
    "    consumer_key = \"RmYMHj3RwVL2qj2sB9V0jbGB2\"\n",
    "    consumer_secret = \"rwkLatJck2pvHEeljerwWfwYi6Wf0Tquy90UyERunzFeXCAlW7\"\n",
    "    access_key = \"590766984-MJRgfipJLzhQt2R6VcktfsANXtiH6Rs87MUpl6ly\"\n",
    "    access_secret = \"P6hl0Lwfclbcz4JjAWb9ILrTkyrPGB6cpd0UVGKmgGcZU\"\n",
    " \n",
    "    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_key, access_secret)\n",
    "    api=tweepy.API(auth,retry_count=3,retry_delay=5,retry_errors=set([401, 404, 500, 503]), wait_on_rate_limit_notify=False ,wait_on_rate_limit=True)\n",
    "    user=api.get_user(UserId)\n",
    "    t=user.screen_name\n",
    "    ids = []\n",
    "    for page in tweepy.Cursor(api.friends_ids, screen_name=t).pages():\n",
    "        ids.extend(page)\n",
    "    return(ids)\n",
    "if __name__=='__main__':\n",
    "    print(friends(119417004))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get friends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#39Indians.txt\t\t5800\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Anfal\\\\Desktop\\\\Fall2019\\\\Social Media Mining\\\\final\\\\rumor detection\\\\Friends/#39Indians.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-ff1b35075aa2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'\\t\\t'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m         \u001b[0mprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-21-ff1b35075aa2>\u001b[0m in \u001b[0;36mprocess\u001b[1;34m(lines, filenames)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfilenames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"\\Friends/\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"w+\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtemp\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[0ms1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\Anfal\\\\Desktop\\\\Fall2019\\\\Social Media Mining\\\\final\\\\rumor detection\\\\Friends/#39Indians.txt'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import codecs\n",
    "\n",
    "def process(lines,filenames):\n",
    "    with open(os.getcwd()+\"\\Friends/\"+filename, \"w+\", encoding='utf-8') as temp:\n",
    "        s1=set()\n",
    "        for i in lines[:]:\n",
    "            if(len(i)>30):\n",
    "                s=i.split('\\t')\n",
    "                s1.add(str(s[1]))\n",
    "        #print(s1)\n",
    "        for i in lines[:]:\n",
    "            if(len(i)>30):\n",
    "                s=i.split('\\t')\n",
    "                temp.write(str(s[1])+\":\\t\")\n",
    "                li=frds.friends(s[1])\n",
    "                print(len(li))\n",
    "                for item in li:\n",
    "                    print(\"%s\\t\" % item)\n",
    "                    if(item in s1):\n",
    "                        temp.write(str(item))\n",
    "                        print(\"%s\\t\" % item)\n",
    "                temp.write(\"\\n\")\n",
    "                \n",
    "for filename in os.listdir(os.getcwd()+'/indian Rumor'):\n",
    "    #print(filename)\n",
    "    with open(\"./indian Rumor/\"+filename,'r', encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "        print(filename+'\\t\\t'+str(int(len(lines)/2)))\n",
    "        process(lines,filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Followers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import json\n",
    "import time\n",
    "\n",
    "with open(\"./data.txt\",'r',encoding='utf-8') as temp:\n",
    "    lines = temp.readlines()\n",
    "    for li in lines:\n",
    "        try:\n",
    "            with open(\"./Followers/\"+str(li)+\".json\",'w+',encoding='utf-8') as temp21:\n",
    "                consumer_key = \"RmYMHj3RwVL2qj2sB9V0jbGB2\"\n",
    "                consumer_secret = \"rwkLatJck2pvHEeljerwWfwYi6Wf0Tquy90UyERunzFeXCAlW7\"\n",
    "                access_key = \"590766984-MJRgfipJLzhQt2R6VcktfsANXtiH6Rs87MUpl6ly\"\n",
    "                access_secret = \"P6hl0Lwfclbcz4JjAWb9ILrTkyrPGB6cpd0UVGKmgGcZU\"\n",
    " \n",
    "                auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "                auth.set_access_token(access_key, access_secret)\n",
    "                api=tweepy.API(auth,retry_count=3,retry_delay=15,retry_errors=set([401, 404, 500, 503]), wait_on_rate_limit_notify=False ,wait_on_rate_limit=True)\n",
    "                user=api.get_user(li)\n",
    "                t=user.screen_name\n",
    "                count=user.followers_count\n",
    "                print(str(li)+\" \"+str(count))\n",
    "                if(count<5000):\n",
    "                    ids = []\n",
    "                    for page in tweepy.Cursor(api.followers_ids, screen_name=t).pages():\n",
    "                        ids.extend(page)\n",
    "                        #time.sleep(60)\n",
    "                    json.dump(ids,temp21)\n",
    "                    #print(\"I am Hero\")\n",
    "                else:\n",
    "                    with open(\"./popularPeople.txt\",'a',encoding='utf-8')as temp3:\n",
    "                        temp3.write(li)\n",
    "        except:\n",
    "            with open(\"./privatePeople.txt\",'a',encoding='utf-8')as temp4:\n",
    "                temp4.write(li)\n",
    "                pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import codecs\n",
    "from langdetect import detect\n",
    "def process(lines,filename):\n",
    "    c=0\n",
    "    hin=0\n",
    "    eng=0\n",
    "    d=0\n",
    "    outputFile = codecs.open(\"otherLang/\"+filename, \"w+\", \"utf-8\")\n",
    "    outputFile.write('id \\t user_id \\t date \\t text\\n')\n",
    "    #outputFile1 = codecs.open(\"english/\"+filename, \"w+\", \"utf-8\")\n",
    "    #outputFile1.write('id \\t user_id \\t date \\t text\\n')\n",
    "    #print(len(lines))\n",
    "    for i in lines[:]:\n",
    "        if(len(i)>10):\n",
    "            s=i.split('\\t')\n",
    "            try:\n",
    "                if(detect(s[3])=='hi'):\n",
    "                    #outputFile.write(i+\"\\n\")\n",
    "                    hin=hin+1\n",
    "                elif(detect(s[3])=='en'):\n",
    "                    #outputFile1.write(i+\"\\n\")\n",
    "                    eng=eng+1\n",
    "                else:\n",
    "                    c=c+1\n",
    "                    outputFile.write(i+\"\\n\")\n",
    "            except:\n",
    "                d=d+1\n",
    "    \n",
    "    outputFile.close()\n",
    "    #outputFile1.close()\n",
    "    print(filename+\" \"+str(hin)+\" \"+str(eng)+\" \"+str(c)+\" \"+str(d))\n",
    "for filename in os.listdir(os.getcwd()+'\\indian Rumor'):\n",
    "    #print(filename)\n",
    "    with open(os.getcwd()+\"\\indian Rumor/\"+filename,'r', encoding=\"utf8\") as f:\n",
    "        lines = f.readlines()\n",
    "        #print(filename+'\\t\\t'+str(int(len(lines)/2)))\n",
    "        process(lines,filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,getopt,datetime,codecs\n",
    "import postid\n",
    "\n",
    "import sys\n",
    "#if sys.version_info[0] < 3:\n",
    "#    import got\n",
    "#else:\n",
    "#    import got3 as got\n",
    "\n",
    "def main1(st):\n",
    "    tweetCriteria = got.manager.TweetCriteria()\n",
    "    tweetCriteria.until ='2018-06-11'\n",
    "    tweetCriteria.since ='2006-06-11'\n",
    "    tweetCriteria.querySearch =st\n",
    "    tweetCriteria.maxTweets=500000\n",
    "    tweetCriteria.querySearch =st\n",
    "    outputFileName =st+'.txt'\n",
    "    outputFile = codecs.open(outputFileName, \"w+\", \"utf-8\")\n",
    "    outputFile.write('id \\t user_id \\t date \\t text')\n",
    "    print('Searching...for  ')\n",
    "    print(st+'\\n')\n",
    "    def receiveBuffer(tweets):\n",
    "        for t in tweets:\n",
    "            outputFile.write(('\\n%s\\t%s\\t%s\\t\"%s\"\\n' % (t.id,postid.get_tweets(t.id),t.date.strftime(\"%Y-%m-%d %H:%M\"), t.text)))\n",
    "        outputFile.flush()\n",
    "        print('More %d saved on file...\\n' % len(tweets))\n",
    "    got.manager.TweetManager.getTweets(tweetCriteria, receiveBuffer)\n",
    "    outputFile.close()\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    x = open(\"rumor.txt\").read().splitlines()\n",
    "    for i in range(0,len(x)):\n",
    "        print(x[i])\n",
    "        main1(x[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
