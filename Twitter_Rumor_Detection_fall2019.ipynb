{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project -fall2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Installed:\n",
    "##### tweepy, json, preprocessor, os, time, codecs\n",
    "\n",
    "\n",
    "#### pip install tweet-preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Posting ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24753438\n"
     ]
    }
   ],
   "source": [
    "import tweepy\n",
    "import json\n",
    "\n",
    "#add users credentials\n",
    "consumer_key = \"RmYMHj3RwVL2qj2sB9V0jbGB2\"\n",
    "consumer_secret = \"rwkLatJck2pvHEeljerwWfwYi6Wf0Tquy90UyERunzFeXCAlW7\"\n",
    "access_key = \"590766984-MJRgfipJLzhQt2R6VcktfsANXtiH6Rs87MUpl6ly\"\n",
    "access_secret = \"P6hl0Lwfclbcz4JjAWb9ILrTkyrPGB6cpd0UVGKmgGcZU\"\n",
    " \n",
    "def get_tweets(id1):\n",
    "        # Authorization to consumer key and consumer secret\n",
    "        auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "        # Access to user's access key and access secret\n",
    "        auth.set_access_token(access_key, access_secret)\n",
    "        # Calling api\n",
    "        api=tweepy.API(auth,retry_count=3,retry_delay=15,retry_errors=set([401, 404, 500, 503]), wait_on_rate_limit_notify=False ,wait_on_rate_limit=True,)\n",
    "        stat=api.get_status(id1)\n",
    "        return stat.user.id\n",
    "# Driver code\n",
    "if __name__ == '__main__':\n",
    " \n",
    "    # Here goes the twitter handle for the user\n",
    "    # whose tweets are to be extracted.\n",
    "    #get_tweets(\"SalmanKhan_\")\n",
    "    print(get_tweets(\"260244087901413376\"));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#39Indians.txt\t\t5800\n",
      "#AadhaarMythBuster.txt\t\t3100\n",
      "#arushi.txt\t\t2100\n",
      "#BJPCambridgeAnalyticaNexus.txt\t\t2563\n",
      "#DeleteNamoApp.txt\t\t6339\n",
      "#DemocracyinDanger.txt\t\t14300\n",
      "#DestroyTheAadhaar.txt\t\t4375\n",
      "#gst.txt\t\t5468\n",
      "#JusticeLoya.txt\t\t4086\n",
      "#Noteban.txt\t\t2616\n",
      "#PNBfraud.txt\t\t2200\n",
      "#Pradyuman.txt\t\t1311\n",
      "#RafelScam.txt\t\t246\n",
      "#SaveTheInternet.txt\t\t304\n",
      "#SSCScam.txt\t\t243\n",
      "10coinInvalid.txt\t\t28\n",
      "2000 note close .txt\t\t393\n",
      "AMUservingfoodhindu.txt\t\t33\n",
      "azadi lakh army Kashmir.txt\t\t110\n",
      "ben kingsley indian.txt\t\t1184\n",
      "Bitcoin ban india.txt\t\t3076\n",
      "BJP’sNupurSharmaAskstoSaveBengalUsing2002RiotsImage.txt\t\t12\n",
      "call 777888999.txt\t\t946\n",
      "caste public discourse .txt\t\t82\n",
      "chennai airport flood .txt\t\t512\n",
      "conversion rate card.txt\t\t183\n",
      "CPM cyber warriors.txt\t\t76\n",
      "dawood property SEIZED.txt\t\t828\n",
      "dhoni liters milk  .txt\t\t19\n",
      "Dhoni Match fixing .txt\t\t2933\n",
      "dhyanchand german.txt\t\t59\n",
      "diamondFirstMinedIndia.txt\t\t320\n",
      "diwali photo space.txt\t\t323\n",
      "dyingWomanMolested.txt\t\t305\n",
      "ElectricityJamaMasjid.txt\t\t636\n",
      "EVMHack .txt\t\t290\n",
      "facebookBff.txt\t\t3078\n",
      "FatwaMenEatWives.txt\t\t1136\n",
      "firstRocketIndiaTransportedCycle.txt\t\t111\n",
      "Freddie Mercury indian.txt\t\t836\n",
      "ganesh drinking milk.txt\t\t237\n",
      "goonchKaliRiver.txt\t\t48\n",
      "gps 2000note .txt\t\t21\n",
      "havells indian brand.txt\t\t16\n",
      "hindi official language.txt\t\t6797\n",
      "hindiOnlyOfficialLanguage.txt\t\t1090\n",
      "india no national game.txt\t\t680\n",
      "india spa elephants.txt\t\t503\n",
      "india women save fuel.txt\t\t27\n",
      "indiaSpaElephants.txt\t\t506\n",
      "inidan govt block porn site .txt\t\t4\n",
      "irfan khan cancer .txt\t\t562\n",
      "itc tobacco company.txt\t\t333\n",
      "JasleenKaurJosanFirstIndianAstronautToJointhe2030MarsMission.txt\t\t1\n",
      "Jayalalitha secret daughter.txt\t\t14\n",
      "JeanDrèzeIndian.txt\t\t241\n",
      "kfc vegetarian for india.txt\t\t85\n",
      "kirpana destroyed.txt\t\t51\n",
      "kovind million followers.txt\t\t182\n",
      "kumbhMelaFromSpace.txt\t\t817\n",
      "lalu daughter harvard lecture.txt\t\t105\n",
      "largest vegetarians india.txt\t\t413\n",
      "lulia salman marriage .txt\t\t634\n",
      "mahatma gandhi dancing images.txt\t\t394\n",
      "mia khalifa bollywood.txt\t\t613\n",
      "milkha looked back .txt\t\t24\n",
      "muslimGirlWonGitaContest.txt\t\t136\n",
      "Muslims arrested Celebrating Win .txt\t\t39\n",
      "Muslims Celebrating pak Win.txt\t\t117\n",
      "NetNeutrality india.txt\t\t5442\n",
      "Nostradamus predicted modi.txt\t\t1022\n",
      "Paresh Mesta murder.txt\t\t457\n",
      "plastic cabbage india.txt\t\t42\n",
      "ponting bat spring .txt\t\t786\n",
      "PrayForChristianPrannoyRoy.txt\t\t6\n",
      "RaveenaTandonToNawazSharif.txt\t\t37\n",
      "Robert Vadra chinese envoy.txt\t\t86\n",
      "salman katrina  married .txt\t\t833\n",
      "salt shortage india.txt\t\t293\n",
      "samsung 9 under display fingerprint.txt\t\t639\n",
      "Senate Elections .txt\t\t345\n",
      "shampooing Indian Concept.txt\t\t93\n",
      "shashi tharoor killed wife.txt\t\t2240\n",
      "SnakesLadderOriginatedIndia.txt\t\t298\n",
      "snapchat snapdeal same.txt\t\t193\n",
      "sridevi death twist .txt\t\t225\n",
      "surya missile .txt\t\t107\n",
      "templeOfRats.txt\t\t3546\n",
      "TripleTalaq .txt\t\t1900\n",
      "under Sasikala Home.txt\t\t25\n",
      "UNESCO Jana Gana Mana best national anthem.txt\t\t852\n",
      "UNESCO Modi best Prime Minister.txt\t\t216\n",
      "VideoClaimingBengaluruAirportWasFlooded.txt\t\t4\n",
      "village name pakistan bihar.txt\t\t23\n",
      "waterMoonDiscoveredIndia.txt\t\t51\n",
      "WhatsAppprofilepictureISIS.txt\t\t7\n",
      "worm dairy milk.txt\t\t411\n",
      "yogi urine.txt\t\t575\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import preprocessor as p\n",
    "#import preprocess as p\n",
    "import sys\n",
    "\n",
    "def process(lines,filenames):\n",
    "    with open(os.getcwd()+\"\\IndianData/\"+filename, \"w+\", encoding='utf-8') as temp:\n",
    "        s1=set()\n",
    "        for i in lines[:]:\n",
    "            if(len(i)>30):\n",
    "                s=i.split('\\t')\n",
    "                s1.add(str(s[1]))\n",
    "        #print(s1)\n",
    "        for i in lines[:]:\n",
    "            if(len(i)>30):\n",
    "                s=i.split('\\t')\n",
    "                temp.write(s[0]+'\\t'+s[1]+\"\\t\"+s[2]+'\\t'+p.clean(s[3])+'\\n')\n",
    "                \n",
    "for filename in os.listdir(os.getcwd()+'/indian Rumor'):\n",
    "    #print(filename)\n",
    "    with open(\"./indian Rumor/\"+filename,'r', encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "        print(filename+'\\t\\t'+str(int(len(lines)/2)))\n",
    "        process(lines,filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Friends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[830360126347309056, 13058232, 48410093, 131249511, 135421739, 87170183, 57928790, 19929890, 18681139, 53555106, 97865628, 56631494, 70179948, 24705126, 101695592, 53790896, 86254626, 27602673, 17753033, 113419517, 114501238, 75854417, 104740065, 87118217]\n"
     ]
    }
   ],
   "source": [
    "import tweepy\n",
    "import time\n",
    "\n",
    "def friends(UserId):\n",
    "    consumer_key = \"RmYMHj3RwVL2qj2sB9V0jbGB2\"\n",
    "    consumer_secret = \"rwkLatJck2pvHEeljerwWfwYi6Wf0Tquy90UyERunzFeXCAlW7\"\n",
    "    access_key = \"590766984-MJRgfipJLzhQt2R6VcktfsANXtiH6Rs87MUpl6ly\"\n",
    "    access_secret = \"P6hl0Lwfclbcz4JjAWb9ILrTkyrPGB6cpd0UVGKmgGcZU\"\n",
    " \n",
    "    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_key, access_secret)\n",
    "    api=tweepy.API(auth,retry_count=3,retry_delay=5,retry_errors=set([401, 404, 500, 503]), wait_on_rate_limit_notify=False ,wait_on_rate_limit=True)\n",
    "    user=api.get_user(UserId)\n",
    "    t=user.screen_name\n",
    "    ids = []\n",
    "    for page in tweepy.Cursor(api.friends_ids, screen_name=t).pages():\n",
    "        ids.extend(page)\n",
    "    return(ids)\n",
    "if __name__=='__main__':\n",
    "    print(friends(119417004))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get friends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#39Indians.txt\t\t5800\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Anfal\\\\Desktop\\\\Fall2019\\\\Social Media Mining\\\\final\\\\rumor detection\\\\Friends/#39Indians.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-ff1b35075aa2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'\\t\\t'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m         \u001b[0mprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-4-ff1b35075aa2>\u001b[0m in \u001b[0;36mprocess\u001b[1;34m(lines, filenames)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfilenames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"\\Friends/\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"w+\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtemp\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[0ms1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\Anfal\\\\Desktop\\\\Fall2019\\\\Social Media Mining\\\\final\\\\rumor detection\\\\Friends/#39Indians.txt'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import codecs\n",
    "\n",
    "def process(lines,filenames):\n",
    "    with open(os.getcwd()+\"\\Friends/\"+filename, \"w+\", encoding='utf-8') as temp:\n",
    "        s1=set()\n",
    "        for i in lines[:]:\n",
    "            if(len(i)>30):\n",
    "                s=i.split('\\t')\n",
    "                s1.add(str(s[1]))\n",
    "        #print(s1)\n",
    "        for i in lines[:]:\n",
    "            if(len(i)>30):\n",
    "                s=i.split('\\t')\n",
    "                temp.write(str(s[1])+\":\\t\")\n",
    "                li=frds.friends(s[1])\n",
    "                print(len(li))\n",
    "                for item in li:\n",
    "                    print(\"%s\\t\" % item)\n",
    "                    if(item in s1):\n",
    "                        temp.write(str(item))\n",
    "                        print(\"%s\\t\" % item)\n",
    "                temp.write(\"\\n\")\n",
    "                \n",
    "for filename in os.listdir(os.getcwd()+'/indian Rumor'):\n",
    "    #print(filename)\n",
    "    with open(\"./indian Rumor/\"+filename,'r', encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "        print(filename+'\\t\\t'+str(int(len(lines)/2)))\n",
    "        process(lines,filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Followers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import json\n",
    "import time\n",
    "\n",
    "with open(\"./data.txt\",'r',encoding='utf-8') as temp:\n",
    "    lines = temp.readlines()\n",
    "    for li in lines:\n",
    "        try:\n",
    "            with open(\"./Followers/\"+str(li)+\".json\",'w+',encoding='utf-8') as temp21:\n",
    "                consumer_key = \"RmYMHj3RwVL2qj2sB9V0jbGB2\"\n",
    "                consumer_secret = \"rwkLatJck2pvHEeljerwWfwYi6Wf0Tquy90UyERunzFeXCAlW7\"\n",
    "                access_key = \"590766984-MJRgfipJLzhQt2R6VcktfsANXtiH6Rs87MUpl6ly\"\n",
    "                access_secret = \"P6hl0Lwfclbcz4JjAWb9ILrTkyrPGB6cpd0UVGKmgGcZU\"\n",
    " \n",
    "                auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "                auth.set_access_token(access_key, access_secret)\n",
    "                api=tweepy.API(auth,retry_count=3,retry_delay=15,retry_errors=set([401, 404, 500, 503]), wait_on_rate_limit_notify=False ,wait_on_rate_limit=True)\n",
    "                user=api.get_user(li)\n",
    "                t=user.screen_name\n",
    "                count=user.followers_count\n",
    "                print(str(li)+\" \"+str(count))\n",
    "                if(count<5000):\n",
    "                    ids = []\n",
    "                    for page in tweepy.Cursor(api.followers_ids, screen_name=t).pages():\n",
    "                        ids.extend(page)\n",
    "                        #time.sleep(60)\n",
    "                    json.dump(ids,temp21)\n",
    "                    #print(\"I am Hero\")\n",
    "                else:\n",
    "                    with open(\"./popularPeople.txt\",'a',encoding='utf-8')as temp3:\n",
    "                        temp3.write(li)\n",
    "        except:\n",
    "            with open(\"./privatePeople.txt\",'a',encoding='utf-8')as temp4:\n",
    "                temp4.write(li)\n",
    "                pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#39Indians.txt 2093 3315 393 0\n",
      "#AadhaarMythBuster.txt 235 2431 435 0\n",
      "#arushi.txt 254 1666 181 0\n",
      "#BJPCambridgeAnalyticaNexus.txt 669 1672 223 0\n",
      "#DeleteNamoApp.txt 1632 3728 980 0\n",
      "#DemocracyinDanger.txt 2133 11109 1059 0\n",
      "#DestroyTheAadhaar.txt 47 3308 1021 0\n",
      "#gst.txt 524 4174 768 3\n",
      "#JusticeLoya.txt 803 2908 375 1\n",
      "#Noteban.txt 289 1912 416 0\n",
      "#PNBfraud.txt 275 1613 312 1\n",
      "#Pradyuman.txt 294 964 54 0\n",
      "#RafelScam.txt 43 159 45 0\n",
      "#SaveTheInternet.txt 0 283 22 0\n",
      "#SSCScam.txt 51 135 58 0\n",
      "10coinInvalid.txt 0 27 2 0\n",
      "2000 note close .txt 0 376 18 0\n",
      "AMUservingfoodhindu.txt 0 33 1 0\n",
      "azadi lakh army Kashmir.txt 0 87 24 0\n",
      "ben kingsley indian.txt 0 1157 28 0\n",
      "Bitcoin ban india.txt 35 2959 83 0\n",
      "BJP’sNupurSharmaAskstoSaveBengalUsing2002RiotsImage.txt 0 12 1 0\n",
      "call 777888999.txt 29 798 118 2\n",
      "caste public discourse .txt 0 82 1 0\n",
      "chennai airport flood .txt 0 500 13 0\n",
      "conversion rate card.txt 0 179 5 0\n",
      "CPM cyber warriors.txt 0 75 2 0\n",
      "dawood property SEIZED.txt 3 816 10 0\n",
      "dhoni liters milk  .txt 0 19 1 0\n",
      "Dhoni Match fixing .txt 0 2617 317 0\n",
      "dhyanchand german.txt 0 59 1 0\n",
      "diamondFirstMinedIndia.txt 0 320 1 0\n",
      "diwali photo space.txt 0 322 2 0\n",
      "dyingWomanMolested.txt 0 305 1 0\n",
      "ElectricityJamaMasjid.txt 0 620 17 0\n",
      "EVMHack .txt 78 157 55 1\n",
      "facebookBff.txt 27 1829 1211 12\n",
      "FatwaMenEatWives.txt 0 1133 4 0\n",
      "firstRocketIndiaTransportedCycle.txt 0 111 1 0\n",
      "Freddie Mercury indian.txt 0 821 16 0\n",
      "ganesh drinking milk.txt 0 222 16 0\n",
      "goonchKaliRiver.txt 0 47 2 0\n",
      "gps 2000note .txt 2 17 3 0\n",
      "havells indian brand.txt 0 16 1 0\n",
      "hindi official language.txt 5 6689 104 0\n",
      "hindiOnlyOfficialLanguage.txt 0 1086 5 0\n",
      "india no national game.txt 0 669 12 0\n",
      "india spa elephants.txt 0 498 6 0\n",
      "india women save fuel.txt 0 27 1 0\n",
      "indiaSpaElephants.txt 0 502 5 0\n",
      "inidan govt block porn site .txt 0 4 1 0\n",
      "irfan khan cancer .txt 15 505 43 0\n",
      "itc tobacco company.txt 1 321 12 0\n",
      "JasleenKaurJosanFirstIndianAstronautToJointhe2030MarsMission.txt 0 1 1 0\n",
      "Jayalalitha secret daughter.txt 0 13 2 0\n",
      "JeanDrèzeIndian.txt 0 237 5 0\n",
      "kfc vegetarian for india.txt 0 80 6 0\n",
      "kirpana destroyed.txt 0 38 14 0\n",
      "kovind million followers.txt 0 181 2 0\n",
      "kumbhMelaFromSpace.txt 0 811 7 0\n",
      "lalu daughter harvard lecture.txt 0 104 2 0\n",
      "largest vegetarians india.txt 0 398 16 0\n",
      "lulia salman marriage .txt 0 587 48 0\n",
      "mahatma gandhi dancing images.txt 0 338 57 0\n",
      "mia khalifa bollywood.txt 2 555 57 0\n",
      "milkha looked back .txt 0 24 1 0\n",
      "muslimGirlWonGitaContest.txt 0 127 10 0\n",
      "Muslims arrested Celebrating Win .txt 0 37 3 0\n",
      "Muslims Celebrating pak Win.txt 0 114 4 0\n",
      "NetNeutrality india.txt 0 5397 46 0\n",
      "Nostradamus predicted modi.txt 4 931 88 0\n",
      "Paresh Mesta murder.txt 0 447 11 0\n",
      "plastic cabbage india.txt 0 41 2 0\n",
      "ponting bat spring .txt 5 623 159 0\n",
      "PrayForChristianPrannoyRoy.txt 0 6 1 0\n",
      "RaveenaTandonToNawazSharif.txt 0 36 2 0\n",
      "Robert Vadra chinese envoy.txt 0 85 2 0\n",
      "salman katrina  married .txt 4 720 110 0\n",
      "salt shortage india.txt 1 288 5 0\n",
      "samsung 9 under display fingerprint.txt 0 618 22 0\n",
      "Senate Elections .txt 0 341 5 0\n",
      "shampooing Indian Concept.txt 0 93 1 0\n",
      "shashi tharoor killed wife.txt 0 2194 47 0\n",
      "SnakesLadderOriginatedIndia.txt 0 298 1 0\n",
      "snapchat snapdeal same.txt 0 184 10 0\n",
      "sridevi death twist .txt 2 218 6 0\n",
      "surya missile .txt 8 92 8 0\n",
      "templeOfRats.txt 9 3388 149 1\n",
      "TripleTalaq .txt 192 1532 177 0\n",
      "under Sasikala Home.txt 0 25 1 0\n",
      "UNESCO Jana Gana Mana best national anthem.txt 0 849 4 0\n",
      "UNESCO Modi best Prime Minister.txt 0 214 3 0\n",
      "VideoClaimingBengaluruAirportWasFlooded.txt 0 4 1 0\n",
      "village name pakistan bihar.txt 0 23 1 0\n",
      "waterMoonDiscoveredIndia.txt 0 51 1 0\n",
      "WhatsAppprofilepictureISIS.txt 0 7 1 0\n",
      "worm dairy milk.txt 1 382 29 0\n",
      "yogi urine.txt 0 486 90 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import codecs\n",
    "from langdetect import detect\n",
    "def process(lines,filename):\n",
    "    c=0\n",
    "    hin=0\n",
    "    eng=0\n",
    "    d=0\n",
    "    outputFile = codecs.open(\"otherLang/\"+filename, \"w+\", \"utf-8\")\n",
    "    outputFile.write('id \\t user_id \\t date \\t text\\n')\n",
    "    #outputFile1 = codecs.open(\"english/\"+filename, \"w+\", \"utf-8\")\n",
    "    #outputFile1.write('id \\t user_id \\t date \\t text\\n')\n",
    "    #print(len(lines))\n",
    "    for i in lines[:]:\n",
    "        if(len(i)>10):\n",
    "            s=i.split('\\t')\n",
    "            try:\n",
    "                if(detect(s[3])=='hi'):\n",
    "                    #outputFile.write(i+\"\\n\")\n",
    "                    hin=hin+1\n",
    "                elif(detect(s[3])=='en'):\n",
    "                    #outputFile1.write(i+\"\\n\")\n",
    "                    eng=eng+1\n",
    "                else:\n",
    "                    c=c+1\n",
    "                    outputFile.write(i+\"\\n\")\n",
    "            except:\n",
    "                d=d+1\n",
    "    \n",
    "    outputFile.close()\n",
    "    #outputFile1.close()\n",
    "    print(filename+\" \"+str(hin)+\" \"+str(eng)+\" \"+str(c)+\" \"+str(d))\n",
    "for filename in os.listdir(os.getcwd()+'\\indian Rumor'):\n",
    "    #print(filename)\n",
    "    with open(os.getcwd()+\"\\indian Rumor/\"+filename,'r', encoding=\"utf8\") as f:\n",
    "        lines = f.readlines()\n",
    "        #print(filename+'\\t\\t'+str(int(len(lines)/2)))\n",
    "        process(lines,filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#DestroyTheAadhaar\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'got' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-2b06d1828ab1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m         \u001b[0mmain1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-7-2b06d1828ab1>\u001b[0m in \u001b[0;36mmain1\u001b[1;34m(st)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmain1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mst\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mtweetCriteria\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmanager\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTweetCriteria\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mtweetCriteria\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muntil\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;34m'2018-06-11'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mtweetCriteria\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msince\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;34m'2006-06-11'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'got' is not defined"
     ]
    }
   ],
   "source": [
    "import sys,getopt,datetime,codecs\n",
    "import sys\n",
    "#if sys.version_info[0] < 3:\n",
    "#    import got\n",
    "#else:\n",
    "#    import got3 as got\n",
    "\n",
    "def main1(st):\n",
    "    tweetCriteria = got.manager.TweetCriteria()\n",
    "    tweetCriteria.until ='2018-06-11'\n",
    "    tweetCriteria.since ='2006-06-11'\n",
    "    tweetCriteria.querySearch =st\n",
    "    tweetCriteria.maxTweets=500000\n",
    "    tweetCriteria.querySearch =st\n",
    "    outputFileName =st+'.txt'\n",
    "    outputFile = codecs.open(outputFileName, \"w+\", \"utf-8\")\n",
    "    outputFile.write('id \\t user_id \\t date \\t text')\n",
    "    print('Searching...for  ')\n",
    "    print(st+'\\n')\n",
    "    def receiveBuffer(tweets):\n",
    "        for t in tweets:\n",
    "            outputFile.write(('\\n%s\\t%s\\t%s\\t\"%s\"\\n' % (t.id,postid.get_tweets(t.id),t.date.strftime(\"%Y-%m-%d %H:%M\"), t.text)))\n",
    "        outputFile.flush()\n",
    "        print('More %d saved on file...\\n' % len(tweets))\n",
    "    got.manager.TweetManager.getTweets(tweetCriteria, receiveBuffer)\n",
    "    outputFile.close()\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    x = open(\"rumor.txt\").read().splitlines()\n",
    "    for i in range(0,len(x)):\n",
    "        print(x[i])\n",
    "        main1(x[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
